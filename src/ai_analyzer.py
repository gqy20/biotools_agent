"""AIåˆ†æå™¨ï¼Œä½¿ç”¨å¤§æ¨¡å‹åˆ†æé¡¹ç›®å†…å®¹"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from .config import config_manager
from .llm_client import LLMClient
from .models import Publication, FunctionalityInfo, UsageInfo, BioToolAnalysis


class AIAnalyzer:
    """AIåˆ†æå™¨"""
    
    def __init__(self, config_override: dict = None):
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient(config_manager)
        print("âœ… AIåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_repository_content(self, repo_path: Path, repo_info, authors) -> BioToolAnalysis:
        """ä½¿ç”¨AIåˆ†æä»“åº“å†…å®¹"""
        
        print("ğŸš€ å¼€å§‹AIå…¨é¢åˆ†æä»“åº“å†…å®¹...")
        print(f"ğŸ“‚ åˆ†æä»“åº“è·¯å¾„: {repo_path}")
        
        # æ”¶é›†READMEæ–‡æ¡£å†…å®¹
        print("ğŸ“ æ”¶é›†READMEæ–‡æ¡£å†…å®¹...")
        readme_content = self._collect_readme_content(repo_path)
        
        if not readme_content:
            print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯")
            return self._create_default_analysis(repo_info, authors)
        
        print(f"âœ… READMEå†…å®¹é•¿åº¦: {len(readme_content)} å­—ç¬¦")
        
        # ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯
        print("ğŸ¤– ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯...")
        analysis_result = self._analyze_all_in_one(readme_content)
        
        # ç»„è£…å®Œæ•´åˆ†æç»“æœ
        print("ğŸ“‹ ç»„è£…å®Œæ•´åˆ†æç»“æœ...")
        analysis = BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=analysis_result["publications"],
            functionality=analysis_result["functionality"],
            usage=analysis_result["usage"],
            analysis_timestamp=datetime.now().isoformat()
        )
        
        print("ğŸ‰ AIåˆ†æå®Œæˆ!")
        print(f"  - å‘è¡¨æ–‡ç« : {len(analysis_result['publications'])} ç¯‡")
        print(f"  - ä¸»è¦åŠŸèƒ½: {analysis_result['functionality'].main_purpose}")
        print(f"  - æ ¸å¿ƒç‰¹æ€§: {len(analysis_result['functionality'].key_features)} ä¸ª")
        
        return analysis
    
    def _collect_readme_content(self, repo_path: Path) -> str:
        """æ”¶é›†READMEæ–‡æ¡£å†…å®¹"""
        
        # READMEæ–‡ä»¶çš„å¯èƒ½å‘½å
        readme_files = [
            "README.md", "README.rst", "README.txt", "README",
            "readme.md", "readme.rst", "readme.txt", "readme",
            "Readme.md", "Readme.rst", "Readme.txt", "Readme"
        ]
        
        for readme_file in readme_files:
            file_path = repo_path / readme_file
            if file_path.exists() and file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file}")
                        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                        return content[:150000] if len(content) > 150000 else content
                except Exception:
                    try:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
                            print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file} (latin-1ç¼–ç )")
                            return content[:150000] if len(content) > 150000 else content
                    except Exception:
                        continue
        
        print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡ä»¶")
        return ""
    
    def _analyze_all_in_one(self, readme_content: str) -> dict:
        """ä¸€æ¬¡æ€§åˆ†ææ‰€æœ‰ä¿¡æ¯"""
        
        # æˆªå–READMEå†…å®¹ï¼Œé¿å…è¿‡é•¿
        content_preview = readme_content[:8000] if len(readme_content) > 8000 else readme_content
        
        prompt = f"""
åˆ†æç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·READMEï¼Œè¿”å›JSONæ ¼å¼ä¿¡æ¯ï¼š

READMEå†…å®¹ï¼š
{content_preview}

è¿”å›JSONæ ¼å¼ï¼š
{{
    "publications": [
        {{"title": "æ–‡ç« æ ‡é¢˜", "authors": ["ä½œè€…"], "journal": "æœŸåˆŠ", "year": å¹´ä»½, "doi": "DOI"}}
    ],
    "functionality": {{
        "main_purpose": "ä¸»è¦ç”¨é€”ä¸€å¥è¯",
        "key_features": ["åŠŸèƒ½1", "åŠŸèƒ½2", "åŠŸèƒ½3"],
        "input_formats": ["è¾“å…¥æ ¼å¼"],
        "output_formats": ["è¾“å‡ºæ ¼å¼"],
        "dependencies": ["ä¾èµ–é¡¹"]
    }},
    "usage": {{
        "installation": "å®‰è£…æ–¹æ³•",
        "basic_usage": "åŸºæœ¬ç”¨æ³•",
        "examples": ["ç¤ºä¾‹1", "ç¤ºä¾‹2"],
        "parameters": ["å‚æ•°è¯´æ˜"]
    }}
}}

è¦æ±‚ï¼š
1. ä»…åŸºäºREADMEå†…å®¹åˆ†æ
2. ç¼ºå¤±ä¿¡æ¯ç”¨ç©ºæ•°ç»„[]æˆ–"æœªè¯´æ˜"
3. é‡ç‚¹è¯†åˆ«ç”Ÿç‰©ä¿¡æ¯å­¦æ ¼å¼(FASTA/BAM/VCFç­‰)
4. ç¡®ä¿æœ‰æ•ˆJSONæ ¼å¼
"""
        
        try:
            # ä½¿ç”¨æ–°çš„LLMå®¢æˆ·ç«¯
            messages = [
                {"role": "system", "content": "You are a helpful assistant specialized in bioinformatics tools analysis. Please respond in the exact JSON format requested."},
                {"role": "user", "content": prompt}
            ]
            
            result = self.llm_client.sync_chat_completion(
                messages=messages,
                max_tokens=2000,
                temperature=0.1,
                timeout=60
            )
            
            # ä½¿ç”¨LLMå®¢æˆ·ç«¯çš„JSONæå–æ–¹æ³•
            data = self.llm_client.extract_json_from_response(result)
            
            if data:
                # è§£æpublications
                publications = []
                for pub_data in data.get("publications", []):
                    pub = Publication(
                        title=pub_data.get("title", ""),
                        authors=pub_data.get("authors", []),
                        journal=pub_data.get("journal"),
                        year=pub_data.get("year"),
                        doi=pub_data.get("doi"),
                        pmid=pub_data.get("pmid")
                    )
                    publications.append(pub)
                
                # è§£æfunctionality
                func_data = data.get("functionality", {})
                functionality = FunctionalityInfo(
                    main_purpose=func_data.get("main_purpose", "ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æå·¥å…·"),
                    key_features=func_data.get("key_features", []),
                    input_formats=func_data.get("input_formats", []),
                    output_formats=func_data.get("output_formats", []),
                    dependencies=func_data.get("dependencies", [])
                )
                
                # è§£æusage
                usage_data = data.get("usage", {})
                usage = UsageInfo(
                    installation=usage_data.get("installation", "è¯·å‚è€ƒé¡¹ç›®READMEæ–‡æ¡£"),
                    basic_usage=usage_data.get("basic_usage", "è¯·æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£è·å–ä½¿ç”¨æ–¹æ³•"),
                    examples=usage_data.get("examples", []),
                    parameters=usage_data.get("parameters", [])
                )
                
                print(f"âœ… æˆåŠŸè§£ææ‰€æœ‰ä¿¡æ¯")
                print(f"  - å‘è¡¨æ–‡ç« : {len(publications)} ç¯‡")
                print(f"  - åŠŸèƒ½ç‰¹æ€§: {len(functionality.key_features)} ä¸ª")
                print(f"  - ä½¿ç”¨ç¤ºä¾‹: {len(usage.examples)} ä¸ª")
                
                return {
                    "publications": publications,
                    "functionality": functionality,
                    "usage": usage
                }
            
        except Exception as e:
            print(f"âš ï¸ AIç»¼åˆåˆ†æå¤±è´¥: {e}")
            print("å°†ä½¿ç”¨é»˜è®¤ä¿¡æ¯...")
        
        # è¿”å›é»˜è®¤å€¼
        return self._get_default_analysis_data()
    
    def _create_default_analysis(self, repo_info, authors) -> BioToolAnalysis:
        """åˆ›å»ºé»˜è®¤åˆ†æç»“æœ"""
        default_data = self._get_default_analysis_data()
        
        return BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=default_data["publications"],
            functionality=default_data["functionality"],
            usage=default_data["usage"],
            analysis_timestamp=datetime.now().isoformat()
        )
    
    def _get_default_analysis_data(self) -> dict:
        """è·å–é»˜è®¤åˆ†ææ•°æ®"""
        return {
            "publications": [],
            "functionality": FunctionalityInfo(
                main_purpose="ç”Ÿç‰©ä¿¡æ¯å­¦åˆ†æå·¥å…·",
                key_features=["æ•°æ®å¤„ç†", "ç»“æœåˆ†æ"],
                input_formats=["æœªåœ¨æ–‡æ¡£ä¸­è¯´æ˜"],
                output_formats=["æœªåœ¨æ–‡æ¡£ä¸­è¯´æ˜"],
                dependencies=["æœªåœ¨æ–‡æ¡£ä¸­è¯´æ˜"]
            ),
            "usage": UsageInfo(
                installation="è¯·å‚è€ƒé¡¹ç›®READMEæ–‡æ¡£",
                basic_usage="è¯·æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£è·å–ä½¿ç”¨æ–¹æ³•",
                examples=["è¯·å‚è€ƒé¡¹ç›®ç¤ºä¾‹"],
                parameters=["è¯·æŸ¥çœ‹å¸®åŠ©æ–‡æ¡£"]
            )
        }
    

