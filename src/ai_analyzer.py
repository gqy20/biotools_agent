"""AIåˆ†æå™¨ï¼Œä½¿ç”¨å¤§æ¨¡å‹åˆ†æé¡¹ç›®å†…å®¹"""

from datetime import datetime
from pathlib import Path
from typing import Optional

from .config import config_manager
from .llm_client import LLMClient
from .models import (
    BioToolAnalysis,
    DataRequirements,
    DeploymentInfo,
    FunctionalityInfo,
    PerformanceInfo,
    Publication,
    TestingInfo,
    UsageInfo,
)


class AIAnalyzer:
    """AIåˆ†æå™¨"""

    def __init__(self, config_override: dict = None):
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient(config_manager)
        print("âœ… AIåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    def analyze_repository_content(
        self, repo_path: Path, repo_info, authors
    ) -> BioToolAnalysis:
        """ä½¿ç”¨AIåˆ†æä»“åº“å†…å®¹"""

        print("ğŸš€ å¼€å§‹AIå…¨é¢åˆ†æä»“åº“å†…å®¹...")
        print(f"ğŸ“‚ åˆ†æä»“åº“è·¯å¾„: {repo_path}")

        # æ”¶é›†READMEæ–‡æ¡£å†…å®¹
        print("ğŸ“ æ”¶é›†READMEæ–‡æ¡£å†…å®¹...")
        readme_content = self._collect_readme_content(repo_path)

        if not readme_content:
            print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯")
            return self._create_default_analysis(repo_info, authors)

        print(f"âœ… READMEå†…å®¹é•¿åº¦: {len(readme_content)} å­—ç¬¦")

        # ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯
        print("ğŸ¤– ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯...")
        analysis_result = self._analyze_all_in_one(readme_content, repo_path)

        # ç»„è£…å®Œæ•´åˆ†æç»“æœ
        print("ğŸ“‹ ç»„è£…å®Œæ•´åˆ†æç»“æœ...")
        analysis = BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=analysis_result["publications"],
            functionality=analysis_result["functionality"],
            usage=analysis_result["usage"],
            architecture=analysis_result.get("architecture"),
            code_quality=analysis_result.get("code_quality"),
            performance=analysis_result.get("performance"),
            bioinformatics_expertise=analysis_result.get("bioinformatics_expertise"),
            usability=analysis_result.get("usability"),
            deployment=analysis_result.get("deployment"),  # æ–°å¢
            testing=analysis_result.get("testing"),  # æ–°å¢
            data_requirements=analysis_result.get("data_requirements"),  # æ–°å¢
            analysis_timestamp=datetime.now().isoformat(),
        )

        print("ğŸ‰ AIåˆ†æå®Œæˆ!")
        print(f"  - å‘è¡¨æ–‡ç« : {len(analysis_result['publications'])} ç¯‡")
        print(f"  - ä¸»è¦åŠŸèƒ½: {analysis_result['functionality'].main_purpose}")
        print(f"  - æ ¸å¿ƒç‰¹æ€§: {len(analysis_result['functionality'].key_features)} ä¸ª")

        return analysis

    def _collect_readme_content(self, repo_path: Path) -> str:
        """æ”¶é›†READMEæ–‡æ¡£å†…å®¹"""

        # READMEæ–‡ä»¶çš„å¯èƒ½å‘½å
        readme_files = [
            "README.md",
            "README.rst",
            "README.txt",
            "README",
            "readme.md",
            "readme.rst",
            "readme.txt",
            "readme",
            "Readme.md",
            "Readme.rst",
            "Readme.txt",
            "Readme",
        ]

        for readme_file in readme_files:
            file_path = repo_path / readme_file
            if file_path.exists() and file_path.is_file():
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file}")
                        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                        return content[:150000] if len(content) > 150000 else content
                except Exception:
                    try:
                        with open(file_path, "r", encoding="latin-1") as f:
                            content = f.read()
                            print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file} (latin-1ç¼–ç )")
                            return (
                                content[:150000] if len(content) > 150000 else content
                            )
                    except Exception:
                        continue

        print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡ä»¶")
        return ""

    def _collect_core_code_samples(self, repo_path: Path) -> str:
        """æ”¶é›†æ ¸å¿ƒä»£ç æ ·æœ¬ - Linusé£æ ¼ï¼šæ‰¾åˆ°ç®—æ³•æ ¸å¿ƒå’Œéƒ¨ç½²æ–‡ä»¶"""
        print("ğŸ” æ”¶é›†æ ¸å¿ƒä»£ç æ ·æœ¬...")

        # æ ¸å¿ƒæ–‡ä»¶æ¨¡å¼ - ç®—æ³•æ–‡ä»¶ + éƒ¨ç½²é…ç½®æ–‡ä»¶
        core_patterns = [
            # ä¸»ç¨‹åºæ–‡ä»¶
            "main.py",
            "main.cpp",
            "main.c",
            "main.java",
            # ç®—æ³•æ ¸å¿ƒ
            "*algorithm*",
            "*core*",
            "*engine*",
            "*align*",
            "*search*",
            "*index*",
            "*parse*",
            # éƒ¨ç½²å’Œé…ç½®æ–‡ä»¶
            "Dockerfile",
            "docker-compose.yml",
            "*.dockerfile",
            "environment.yml",
            "conda.yml",
            "requirements.txt",
            "setup.py",
            "setup.cfg",
            "pyproject.toml",
            "Makefile",
            "CMakeLists.txt",
            "test_*.py",
            "*_test.py",
            "test*.sh",
            "*.py",
            "*.cpp",
            "*.c",
            "*.java",
            "*.R",
        ]

        code_samples = []
        file_count = 0
        max_files = 8  # å¢åŠ æ–‡ä»¶æ•°é‡ä»¥åŒ…å«æ›´å¤šéƒ¨ç½²ä¿¡æ¯
        max_content = 1500  # å‡å°‘æ¯ä¸ªæ–‡ä»¶å†…å®¹ä»¥è…¾å‡ºç©ºé—´

        for pattern in core_patterns:
            if file_count >= max_files:
                break

            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            try:
                for file_path in repo_path.rglob(pattern):
                    if file_count >= max_files:
                        break

                    # è·³è¿‡ä¸ç›¸å…³ç›®å½•ï¼Œä½†ä¿ç•™testç›®å½•ï¼ˆç”¨äºåˆ†ææµ‹è¯•ä¿¡æ¯ï¼‰
                    if any(
                        skip in str(file_path)
                        for skip in [".git", "__pycache__", "doc", "example"]
                    ):
                        continue

                    if (
                        file_path.is_file() and file_path.stat().st_size < 50000
                    ):  # å°äº50KB
                        try:
                            with open(
                                file_path, "r", encoding="utf-8", errors="ignore"
                            ) as f:
                                content = f.read()[:max_content]
                                if content.strip():
                                    relative_path = file_path.relative_to(repo_path)
                                    code_samples.append(
                                        f"=== {relative_path} ===\n{content}\n"
                                    )
                                    file_count += 1
                                    print(f"ğŸ“„ æ”¶é›†ä»£ç æ–‡ä»¶: {relative_path}")
                        except Exception:
                            continue
            except Exception:
                continue

        result = "\n".join(code_samples)
        print(f"âœ… æ”¶é›†äº† {file_count} ä¸ªæ ¸å¿ƒä»£ç æ–‡ä»¶ï¼Œæ€»é•¿åº¦: {len(result)} å­—ç¬¦")
        return result

    def _build_analysis_prompt(
        self, readme_content: str, code_content: str = ""
    ) -> str:
        """æ„å»ºåˆ†æç”¨çš„prompt - Linusé£æ ¼ï¼šæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ"""
        # æˆªå–READMEå†…å®¹ï¼Œé¿å…è¿‡é•¿
        content_preview = (
            readme_content[:6000] if len(readme_content) > 6000 else readme_content
        )
        code_preview = code_content[:4000] if len(code_content) > 4000 else code_content

        prompt = "åˆ†æè¿™ä¸ªç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„READMEæ–‡æ¡£"

        if code_content:
            prompt += "å’Œæ ¸å¿ƒä»£ç "

        prompt += f"""ï¼Œæå–å…¶ä¸­çš„äº‹å®ä¿¡æ¯ã€‚æ‰€æœ‰å›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚

READMEå†…å®¹ï¼š
{content_preview}"""

        if code_content:
            prompt += f"""

æ ¸å¿ƒä»£ç ç‰‡æ®µï¼š
{code_preview}"""

        prompt += """

è¿”å›JSONæ ¼å¼ï¼Œä»…åŒ…å«æ˜ç¡®æåˆ°æˆ–å¯ä»¥ä»ä»£ç ä¸­åˆ†æå‡ºçš„ä¿¡æ¯ï¼š

{
    "publications": [
        // ä»…å½“READMEæ˜ç¡®æåˆ°è®ºæ–‡æ ‡é¢˜æ—¶æ‰åŒ…å«
        {"title": "READMEä¸­çš„ç¡®åˆ‡æ ‡é¢˜", "journal": "å¦‚æœæåˆ°æœŸåˆŠå", "year": å¹´ä»½æ•°å­—, "doi": "å¦‚æœæœ‰DOI"}
    ],
    "functionality": {
        "main_purpose": "ç”¨ä¸€å¥ä¸­æ–‡æè¿°æ­¤å·¥å…·çš„ç”¨é€”",
        "key_features": ["åŠŸèƒ½ç‰¹ç‚¹1", "åŠŸèƒ½ç‰¹ç‚¹2"],  // ä»…READMEæ˜ç¡®æåˆ°çš„åŠŸèƒ½
        "input_formats": ["FASTA", "BAM"],  // ä»…æ˜ç¡®æåˆ°çš„è¾“å…¥æ ¼å¼
        "output_formats": ["GFF", "VCF"],   // ä»…æ˜ç¡®æåˆ°çš„è¾“å‡ºæ ¼å¼
        "dependencies": ["Python", "BWA"]   // ä»…æ˜ç¡®æåˆ°çš„ä¾èµ–
    },
    "usage": {
        "installation": "READMEä¸­çš„ç¡®åˆ‡å®‰è£…å‘½ä»¤",
        "basic_usage": "åŸºæœ¬ä½¿ç”¨å‘½ä»¤",
        "examples": ["ç¤ºä¾‹1", "ç¤ºä¾‹2"]
    },
    "performance": {
        "algorithm_complexity": "åŸºäºä»£ç åˆ†æçš„ç®—æ³•å¤æ‚åº¦",
        "resource_requirements": "èµ„æºéœ€æ±‚åˆ†æ",
        "optimization_features": "å‘ç°çš„ä¼˜åŒ–ç‰¹æ€§"
    },
    "deployment": {
        "installation_methods": ["conda", "pip", "docker"],  // æ˜ç¡®æåˆ°çš„å®‰è£…æ–¹å¼
        "system_requirements": ["Linux", "Python 3.8+"],    // ç³»ç»Ÿè¦æ±‚
        "container_support": ["Docker", "Singularity"],      // å®¹å™¨æ”¯æŒ
        "cloud_deployment": ["AWS", "Google Cloud"],         // äº‘éƒ¨ç½²é€‰é¡¹
        "configuration_files": ["config.yaml", ".env"]       // é…ç½®æ–‡ä»¶
    },
    "testing": {
        "test_commands": ["python -m pytest", "make test"],  // æµ‹è¯•å‘½ä»¤
        "test_data_sources": ["ç¤ºä¾‹æ•°æ®URL", "æµ‹è¯•æ•°æ®é›†"],    // æµ‹è¯•æ•°æ®æ¥æº
        "example_datasets": ["example.fasta", "demo.bam"],   // ç¤ºä¾‹æ•°æ®
        "validation_methods": ["åŸºå‡†æ¯”è¾ƒ", "å·²çŸ¥ç»“æœéªŒè¯"],     // éªŒè¯æ–¹æ³•
        "benchmark_datasets": ["æ ‡å‡†æ•°æ®é›†åç§°"]              // åŸºå‡†æ•°æ®é›†
    },
    "data_requirements": {
        "required_inputs": ["åŸºå› ç»„åºåˆ—", "æ³¨é‡Šæ–‡ä»¶"],         // å¿…éœ€è¾“å…¥
        "optional_inputs": ["è´¨é‡æ–‡ä»¶", "æ©ç æ–‡ä»¶"],           // å¯é€‰è¾“å…¥
        "data_formats": ["FASTA", "GFF3", "BED"],           // æ”¯æŒæ ¼å¼
        "file_size_limits": "æœ€å¤§æ–‡ä»¶å¤§å°é™åˆ¶",               // å¤§å°é™åˆ¶
        "preprocessing_steps": ["è´¨é‡è¿‡æ»¤", "æ ¼å¼è½¬æ¢"]        // é¢„å¤„ç†æ­¥éª¤
    }
}

ä¸¥æ ¼è¦æ±‚ï¼š
1. æ‰€æœ‰æ–‡æœ¬å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾
2. ä»…æå–README/ä»£ç ä¸­æ˜ç¡®å†™æ˜çš„ä¿¡æ¯
3. ç‰¹åˆ«å…³æ³¨å®‰è£…è¯´æ˜ã€æµ‹è¯•ç¤ºä¾‹ã€æ•°æ®è¦æ±‚éƒ¨åˆ†
4. å¦‚æœä¿¡æ¯ç¼ºå¤±ï¼Œç›´æ¥çœç•¥è¯¥å­—æ®µ
5. ç»ä¸ä½¿ç”¨å ä½ç¬¦æˆ–æ¨¡æ¿æ–‡æœ¬
6. å¯¹äºéƒ¨ç½²ä¿¡æ¯ï¼Œé‡ç‚¹æŸ¥æ‰¾Dockerã€condaã€pipç­‰å…³é”®è¯
7. å¯¹äºæµ‹è¯•ä¿¡æ¯ï¼ŒæŸ¥æ‰¾testã€exampleã€demoç­‰ç›¸å…³å†…å®¹
8. è¿”å›ç®€æ´ã€å®ç”¨çš„ä¸­æ–‡JSON"""

        return prompt

    def _call_llm_for_analysis(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMè¿›è¡Œåˆ†æ"""
        try:
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“é—¨åˆ†æç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼å›ç­”ï¼Œæ‰€æœ‰å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚",
                },
                {"role": "user", "content": prompt},
            ]

            return self.llm_client.sync_chat_completion(
                messages=messages, max_tokens=3000, temperature=0.1, timeout=60
            )
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None

    def _parse_analysis_result(self, llm_response: str) -> dict:
        """è§£æLLMè¿”å›çš„åˆ†æç»“æœ - Linusé£æ ¼: æ¶ˆé™¤å¤æ‚åº¦"""
        data = self.llm_client.extract_json_from_response(llm_response)

        if not data:
            print("âš ï¸ æœªèƒ½è·å–æœ‰æ•ˆçš„åˆ†æç»“æœï¼Œä½¿ç”¨æœ€å°é»˜è®¤å€¼")
            return self._get_minimal_defaults()

        # ç®€å•ç›´æ¥çš„è§£æ - ä¸è¦è¿‡åº¦å¤„ç†
        publications = [
            Publication(
                title=pub.get("title", ""),
                authors=pub.get("authors", []),
                journal=pub.get("journal"),
                year=pub.get("year"),
                doi=pub.get("doi"),
            )
            for pub in data.get("publications", [])
            if pub.get("title")  # åªæœ‰titleå­˜åœ¨æ‰åˆ›å»º
        ]

        # åŠŸèƒ½ä¿¡æ¯ - ç®€å•è·å–ï¼Œæ²¡æœ‰å¤æ‚çš„é»˜è®¤å€¼å¤„ç†
        func_data = data.get("functionality", {})
        functionality = FunctionalityInfo(
            main_purpose=func_data.get("main_purpose", "ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·"),
            key_features=func_data.get("key_features", []),
            input_formats=func_data.get("input_formats", []),
            output_formats=func_data.get("output_formats", []),
            dependencies=func_data.get("dependencies", []),
        )

        # ä½¿ç”¨ä¿¡æ¯ - æœ€ç®€å®ç°
        usage_data = data.get("usage", {})
        usage = UsageInfo(
            installation=usage_data.get("installation", "å‚è€ƒREADME"),
            basic_usage=usage_data.get("basic_usage", "å‚è€ƒREADME"),
            examples=usage_data.get("examples", []),
            parameters=usage_data.get("parameters", []),
        )

        # æ€§èƒ½ä¿¡æ¯ - åŸºäºä»£ç å’ŒREADMEçš„ç»¼åˆåˆ†æ
        performance_data = data.get("performance", {})
        performance = None
        if performance_data:
            # å®‰å…¨åœ°å¤„ç†å¯èƒ½æ˜¯æ•°ç»„çš„å­—æ®µ
            def safe_get_string(data_dict, key, default=""):
                value = data_dict.get(key, default)
                if isinstance(value, list):
                    return " ".join(str(v) for v in value) if value else default
                return str(value) if value else default

            performance = PerformanceInfo(
                time_complexity=safe_get_string(
                    performance_data, "algorithm_complexity"
                ),
                space_complexity=safe_get_string(
                    performance_data, "resource_requirements"
                ),
                parallelization=safe_get_string(
                    performance_data, "optimization_features"
                ),
                resource_usage=safe_get_string(
                    performance_data, "resource_requirements"
                ),
                optimization_suggestions=[],
            )

        # éƒ¨ç½²ä¿¡æ¯ - Linusé£æ ¼ï¼šå®ç”¨ä¸ºä¸»
        deployment_data = data.get("deployment", {})
        deployment = None
        if deployment_data:
            deployment = DeploymentInfo(
                installation_methods=deployment_data.get("installation_methods", []),
                system_requirements=deployment_data.get("system_requirements", []),
                container_support=deployment_data.get("container_support", []),
                cloud_deployment=deployment_data.get("cloud_deployment", []),
                configuration_files=deployment_data.get("configuration_files", []),
            )

        # æµ‹è¯•ä¿¡æ¯ - Linusé£æ ¼ï¼šå¯æ‰§è¡Œçš„æŒ‡å¯¼
        testing_data = data.get("testing", {})
        testing = None
        if testing_data:
            testing = TestingInfo(
                test_commands=testing_data.get("test_commands", []),
                test_data_sources=testing_data.get("test_data_sources", []),
                example_datasets=testing_data.get("example_datasets", []),
                validation_methods=testing_data.get("validation_methods", []),
                benchmark_datasets=testing_data.get("benchmark_datasets", []),
            )

        # æ•°æ®éœ€æ±‚ - Linusé£æ ¼ï¼šæ˜ç¡®å…·ä½“
        data_req_data = data.get("data_requirements", {})
        data_requirements = None
        if data_req_data:
            data_requirements = DataRequirements(
                required_inputs=data_req_data.get("required_inputs", []),
                optional_inputs=data_req_data.get("optional_inputs", []),
                data_formats=data_req_data.get("data_formats", []),
                file_size_limits=data_req_data.get("file_size_limits", ""),
                preprocessing_steps=data_req_data.get("preprocessing_steps", []),
            )

        return {
            "publications": publications,
            "functionality": functionality,
            "usage": usage,
            "performance": performance,
            "deployment": deployment,  # æ–°å¢
            "testing": testing,  # æ–°å¢
            "data_requirements": data_requirements,  # æ–°å¢
            "code_quality": None,  # ç æ‰ä¸å¿…è¦çš„å¤æ‚æ€§
            "bioinformatics_expertise": None,
            "usability": None,
        }

    def _get_minimal_defaults(self) -> dict:
        """è·å–æœ€å°é»˜è®¤æ•°æ® - Linusé£æ ¼: ç®€å•ç›´æ¥"""
        return {
            "publications": [],
            "functionality": FunctionalityInfo(
                main_purpose="ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·",
                key_features=[],
                input_formats=[],
                output_formats=[],
                dependencies=[],
            ),
            "usage": UsageInfo(
                installation="å‚è€ƒREADME",
                basic_usage="å‚è€ƒREADME",
                examples=[],
                parameters=[],
            ),
            "performance": None,
            "deployment": None,
            "testing": None,
            "data_requirements": None,
            "code_quality": None,
            "bioinformatics_expertise": None,
            "usability": None,
        }

    def _create_default_analysis(self, repo_info, authors) -> BioToolAnalysis:
        """åˆ›å»ºé»˜è®¤åˆ†æç»“æœ"""
        defaults = self._get_minimal_defaults()

        return BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=defaults["publications"],
            functionality=defaults["functionality"],
            usage=defaults["usage"],
            deployment=defaults["deployment"],
            testing=defaults["testing"],
            data_requirements=defaults["data_requirements"],
            analysis_timestamp=datetime.now().isoformat(),
        )

    def _analyze_all_in_one(self, readme_content: str, repo_path: Path) -> dict:
        """ä¸€æ¬¡æ€§åˆ†æ - Linusé£æ ¼ï¼šç®€å•é«˜æ•ˆ"""
        # 1. æ”¶é›†ä»£ç æ ·æœ¬ç”¨äºæ·±åº¦åˆ†æ
        code_content = self._collect_core_code_samples(repo_path)

        # 2. æ„å»ºåŒ…å«ä»£ç çš„prompt
        prompt = self._build_analysis_prompt(readme_content, code_content)

        # 3. è°ƒç”¨LLM
        llm_response = self._call_llm_for_analysis(prompt)
        if not llm_response:
            return self._get_minimal_defaults()

        # 4. è§£æç»“æœ
        return self._parse_analysis_result(llm_response)
