"""AIåˆ†æå™¨ï¼Œä½¿ç”¨å¤§æ¨¡å‹åˆ†æé¡¹ç›®å†…å®¹"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional

from .config import config_manager
from .llm_client import LLMClient
from .models import Publication, FunctionalityInfo, UsageInfo, BioToolAnalysis, CodeQualityInfo, PerformanceInfo, BioinformaticsExpertiseInfo, UsabilityInfo


class AIAnalyzer:
    """AIåˆ†æå™¨"""
    
    def __init__(self, config_override: dict = None):
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = LLMClient(config_manager)
        print("âœ… AIåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_repository_content(self, repo_path: Path, repo_info, authors) -> BioToolAnalysis:
        """ä½¿ç”¨AIåˆ†æä»“åº“å†…å®¹"""
        
        print("ğŸš€ å¼€å§‹AIå…¨é¢åˆ†æä»“åº“å†…å®¹...")
        print(f"ğŸ“‚ åˆ†æä»“åº“è·¯å¾„: {repo_path}")
        
        # æ”¶é›†READMEæ–‡æ¡£å†…å®¹
        print("ğŸ“ æ”¶é›†READMEæ–‡æ¡£å†…å®¹...")
        readme_content = self._collect_readme_content(repo_path)
        
        if not readme_content:
            print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡æ¡£ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯")
            return self._create_default_analysis(repo_info, authors)
        
        print(f"âœ… READMEå†…å®¹é•¿åº¦: {len(readme_content)} å­—ç¬¦")
        
        # ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯
        print("ğŸ¤– ä¸€æ¬¡æ€§AIåˆ†æè·å–æ‰€æœ‰ä¿¡æ¯...")
        analysis_result = self._analyze_all_in_one(readme_content, repo_path)
        
        # ç»„è£…å®Œæ•´åˆ†æç»“æœ
        print("ğŸ“‹ ç»„è£…å®Œæ•´åˆ†æç»“æœ...")
        analysis = BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=analysis_result["publications"],
            functionality=analysis_result["functionality"],
            usage=analysis_result["usage"],
            architecture=analysis_result.get("architecture"),
            code_quality=analysis_result.get("code_quality"),
            performance=analysis_result.get("performance"),
            bioinformatics_expertise=analysis_result.get("bioinformatics_expertise"),
            usability=analysis_result.get("usability"),
            analysis_timestamp=datetime.now().isoformat()
        )
        
        print("ğŸ‰ AIåˆ†æå®Œæˆ!")
        print(f"  - å‘è¡¨æ–‡ç« : {len(analysis_result['publications'])} ç¯‡")
        print(f"  - ä¸»è¦åŠŸèƒ½: {analysis_result['functionality'].main_purpose}")
        print(f"  - æ ¸å¿ƒç‰¹æ€§: {len(analysis_result['functionality'].key_features)} ä¸ª")
        
        return analysis
    
    def _collect_readme_content(self, repo_path: Path) -> str:
        """æ”¶é›†READMEæ–‡æ¡£å†…å®¹"""
        
        # READMEæ–‡ä»¶çš„å¯èƒ½å‘½å
        readme_files = [
            "README.md", "README.rst", "README.txt", "README",
            "readme.md", "readme.rst", "readme.txt", "readme",
            "Readme.md", "Readme.rst", "Readme.txt", "Readme"
        ]
        
        for readme_file in readme_files:
            file_path = repo_path / readme_file
            if file_path.exists() and file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file}")
                        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…è¿‡é•¿
                        return content[:150000] if len(content) > 150000 else content
                except Exception:
                    try:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
                            print(f"ğŸ“„ æ‰¾åˆ°READMEæ–‡ä»¶: {readme_file} (latin-1ç¼–ç )")
                            return content[:150000] if len(content) > 150000 else content
                    except Exception:
                        continue
        
        print("âš ï¸ æœªæ‰¾åˆ°READMEæ–‡ä»¶")
        return ""
    
    def _collect_core_code_samples(self, repo_path: Path) -> str:
        """æ”¶é›†æ ¸å¿ƒä»£ç æ ·æœ¬ - Linusé£æ ¼ï¼šæ‰¾åˆ°ç®—æ³•æ ¸å¿ƒ"""
        print("ğŸ” æ”¶é›†æ ¸å¿ƒä»£ç æ ·æœ¬...")
        
        # æ ¸å¿ƒæ–‡ä»¶æ¨¡å¼ - ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·å¸¸è§çš„æ ¸å¿ƒæ–‡ä»¶
        core_patterns = [
            "main.py", "main.cpp", "main.c", "main.java",
            "*algorithm*", "*core*", "*engine*", 
            "*align*", "*search*", "*index*", "*parse*",
            "*.py", "*.cpp", "*.c", "*.java", "*.R"
        ]
        
        code_samples = []
        file_count = 0
        max_files = 5  # é™åˆ¶æ–‡ä»¶æ•°é‡
        max_content = 2000  # æ¯ä¸ªæ–‡ä»¶æœ€å¤§å†…å®¹é•¿åº¦
        
        for pattern in core_patterns:
            if file_count >= max_files:
                break
                
            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            try:
                for file_path in repo_path.rglob(pattern):
                    if file_count >= max_files:
                        break
                        
                    # è·³è¿‡ä¸ç›¸å…³ç›®å½•
                    if any(skip in str(file_path) for skip in ['.git', '__pycache__', 'test', 'doc', 'example']):
                        continue
                        
                    if file_path.is_file() and file_path.stat().st_size < 50000:  # å°äº50KB
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read()[:max_content]
                                if content.strip():
                                    relative_path = file_path.relative_to(repo_path)
                                    code_samples.append(f"=== {relative_path} ===\n{content}\n")
                                    file_count += 1
                                    print(f"ğŸ“„ æ”¶é›†ä»£ç æ–‡ä»¶: {relative_path}")
                        except Exception:
                            continue
            except Exception:
                continue
        
        result = "\n".join(code_samples)
        print(f"âœ… æ”¶é›†äº† {file_count} ä¸ªæ ¸å¿ƒä»£ç æ–‡ä»¶ï¼Œæ€»é•¿åº¦: {len(result)} å­—ç¬¦")
        return result
    
    def _build_analysis_prompt(self, readme_content: str, code_content: str = "") -> str:
        """æ„å»ºåˆ†æç”¨çš„prompt - Linusé£æ ¼ï¼šæ¶ˆé™¤ç‰¹æ®Šæƒ…å†µ"""
        # æˆªå–READMEå†…å®¹ï¼Œé¿å…è¿‡é•¿
        content_preview = readme_content[:6000] if len(readme_content) > 6000 else readme_content
        code_preview = code_content[:4000] if len(code_content) > 4000 else code_content
        
        prompt = f"""åˆ†æè¿™ä¸ªç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„READMEæ–‡æ¡£"""
        
        if code_content:
            prompt += "å’Œæ ¸å¿ƒä»£ç "
            
        prompt += f"""ï¼Œæå–å…¶ä¸­çš„äº‹å®ä¿¡æ¯ã€‚æ‰€æœ‰å›ç­”å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚

READMEå†…å®¹ï¼š
{content_preview}"""

        if code_content:
            prompt += f"""

æ ¸å¿ƒä»£ç ç‰‡æ®µï¼š
{code_preview}"""

        prompt += """

è¿”å›JSONæ ¼å¼ï¼Œä»…åŒ…å«æ˜ç¡®æåˆ°æˆ–å¯ä»¥ä»ä»£ç ä¸­åˆ†æå‡ºçš„ä¿¡æ¯ï¼š

{
    "publications": [
        // ä»…å½“READMEæ˜ç¡®æåˆ°è®ºæ–‡æ ‡é¢˜æ—¶æ‰åŒ…å«
        {"title": "READMEä¸­çš„ç¡®åˆ‡æ ‡é¢˜", "journal": "å¦‚æœæåˆ°æœŸåˆŠå", "year": å¹´ä»½æ•°å­—, "doi": "å¦‚æœæœ‰DOI"}
    ],
    "functionality": {
        "main_purpose": "ç”¨ä¸€å¥ä¸­æ–‡æè¿°æ­¤å·¥å…·çš„ç”¨é€”",
        "key_features": ["åŠŸèƒ½ç‰¹ç‚¹1", "åŠŸèƒ½ç‰¹ç‚¹2"],  // ä»…READMEæ˜ç¡®æåˆ°çš„åŠŸèƒ½
        "input_formats": ["FASTA", "BAM"],  // ä»…æ˜ç¡®æåˆ°çš„è¾“å…¥æ ¼å¼
        "output_formats": ["GFF", "VCF"],   // ä»…æ˜ç¡®æåˆ°çš„è¾“å‡ºæ ¼å¼
        "dependencies": ["Python", "BWA"]   // ä»…æ˜ç¡®æåˆ°çš„ä¾èµ–
    },
    "usage": {
        "installation": "READMEä¸­çš„ç¡®åˆ‡å®‰è£…å‘½ä»¤",
        "basic_usage": "åŸºæœ¬ä½¿ç”¨å‘½ä»¤",
        "examples": ["ç¤ºä¾‹1", "ç¤ºä¾‹2"]
    },
    "performance": {
        "algorithm_complexity": "åŸºäºä»£ç åˆ†æçš„ç®—æ³•å¤æ‚åº¦",
        "resource_requirements": "èµ„æºéœ€æ±‚åˆ†æ", 
        "optimization_features": "å‘ç°çš„ä¼˜åŒ–ç‰¹æ€§"
    }
}

ä¸¥æ ¼è¦æ±‚ï¼š
1. æ‰€æœ‰æ–‡æœ¬å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾
2. ä»…æå–READMEä¸­æ˜ç¡®å†™æ˜çš„ä¿¡æ¯
3. å¦‚æœæä¾›äº†ä»£ç ï¼Œç»“åˆä»£ç è¿›è¡Œç®—æ³•å¤æ‚åº¦åˆ†æ
4. å¦‚æœä¿¡æ¯ç¼ºå¤±ï¼Œç›´æ¥çœç•¥è¯¥å­—æ®µ
5. ç»ä¸ä½¿ç”¨å ä½ç¬¦æˆ–æ¨¡æ¿æ–‡æœ¬
6. è¿”å›ç®€æ´ã€äº‹å®æ€§çš„ä¸­æ–‡JSON"""

        return prompt

    def _call_llm_for_analysis(self, prompt: str) -> Optional[str]:
        """è°ƒç”¨LLMè¿›è¡Œåˆ†æ"""
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸“é—¨åˆ†æç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·çš„åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼å›ç­”ï¼Œæ‰€æœ‰å†…å®¹å¿…é¡»ä½¿ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            return self.llm_client.sync_chat_completion(
                messages=messages,
                max_tokens=3000,
                temperature=0.1,
                timeout=60
            )
        except Exception as e:
            print(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return None

    def _parse_analysis_result(self, llm_response: str) -> dict:
        """è§£æLLMè¿”å›çš„åˆ†æç»“æœ - Linusé£æ ¼: æ¶ˆé™¤å¤æ‚åº¦"""
        data = self.llm_client.extract_json_from_response(llm_response)
        
        if not data:
            print("âš ï¸ æœªèƒ½è·å–æœ‰æ•ˆçš„åˆ†æç»“æœï¼Œä½¿ç”¨æœ€å°é»˜è®¤å€¼")
            return self._get_minimal_defaults()
        
        # ç®€å•ç›´æ¥çš„è§£æ - ä¸è¦è¿‡åº¦å¤„ç†
        publications = [
            Publication(
                title=pub.get("title", ""),
                authors=pub.get("authors", []),
                journal=pub.get("journal"),
                year=pub.get("year"),
                doi=pub.get("doi")
            )
            for pub in data.get("publications", [])
            if pub.get("title")  # åªæœ‰titleå­˜åœ¨æ‰åˆ›å»º
        ]
        
        # åŠŸèƒ½ä¿¡æ¯ - ç®€å•è·å–ï¼Œæ²¡æœ‰å¤æ‚çš„é»˜è®¤å€¼å¤„ç†
        func_data = data.get("functionality", {})
        functionality = FunctionalityInfo(
            main_purpose=func_data.get("main_purpose", "ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·"),
            key_features=func_data.get("key_features", []),
            input_formats=func_data.get("input_formats", []),
            output_formats=func_data.get("output_formats", []),
            dependencies=func_data.get("dependencies", [])
        )
        
        # ä½¿ç”¨ä¿¡æ¯ - æœ€ç®€å®ç°
        usage_data = data.get("usage", {})
        usage = UsageInfo(
            installation=usage_data.get("installation", "å‚è€ƒREADME"),
            basic_usage=usage_data.get("basic_usage", "å‚è€ƒREADME"),
            examples=usage_data.get("examples", []),
            parameters=usage_data.get("parameters", [])
        )
        
        # æ€§èƒ½ä¿¡æ¯ - åŸºäºä»£ç å’ŒREADMEçš„ç»¼åˆåˆ†æ
        performance_data = data.get("performance", {})
        performance = None
        if performance_data:
            # å®‰å…¨åœ°å¤„ç†å¯èƒ½æ˜¯æ•°ç»„çš„å­—æ®µ
            def safe_get_string(data_dict, key, default=""):
                value = data_dict.get(key, default)
                if isinstance(value, list):
                    return " ".join(str(v) for v in value) if value else default
                return str(value) if value else default
            
            performance = PerformanceInfo(
                time_complexity=safe_get_string(performance_data, "algorithm_complexity"),
                space_complexity=safe_get_string(performance_data, "resource_requirements"),
                parallelization=safe_get_string(performance_data, "optimization_features"),
                resource_usage=safe_get_string(performance_data, "resource_requirements"),
                optimization_suggestions=[]
            )
        
        return {
            "publications": publications,
            "functionality": functionality,
            "usage": usage,
            "performance": performance,  # ç°åœ¨åŒ…å«çœŸå®çš„æ€§èƒ½åˆ†æ
            "code_quality": None,  # ç æ‰ä¸å¿…è¦çš„å¤æ‚æ€§
            "bioinformatics_expertise": None,
            "usability": None
        }

    def _get_minimal_defaults(self) -> dict:
        """è·å–æœ€å°é»˜è®¤æ•°æ® - Linusé£æ ¼: ç®€å•ç›´æ¥"""
        return {
            "publications": [],
            "functionality": FunctionalityInfo(
                main_purpose="ç”Ÿç‰©ä¿¡æ¯å­¦å·¥å…·",
                key_features=[],
                input_formats=[],
                output_formats=[],
                dependencies=[]
            ),
            "usage": UsageInfo(
                installation="å‚è€ƒREADME",
                basic_usage="å‚è€ƒREADME", 
                examples=[],
                parameters=[]
            ),
            "code_quality": None,
            "performance": None,
            "bioinformatics_expertise": None,
            "usability": None
        }

    def _create_default_analysis(self, repo_info, authors) -> BioToolAnalysis:
        """åˆ›å»ºé»˜è®¤åˆ†æç»“æœ"""
        defaults = self._get_minimal_defaults()
        
        return BioToolAnalysis(
            repository=repo_info,
            authors=authors,
            publications=defaults["publications"],
            functionality=defaults["functionality"],
            usage=defaults["usage"],
            analysis_timestamp=datetime.now().isoformat()
        )

    def _analyze_all_in_one(self, readme_content: str, repo_path: Path) -> dict:
        """ä¸€æ¬¡æ€§åˆ†æ - Linusé£æ ¼ï¼šç®€å•é«˜æ•ˆ"""
        # 1. æ”¶é›†ä»£ç æ ·æœ¬ç”¨äºæ·±åº¦åˆ†æ
        code_content = self._collect_core_code_samples(repo_path)
        
        # 2. æ„å»ºåŒ…å«ä»£ç çš„prompt
        prompt = self._build_analysis_prompt(readme_content, code_content)
        
        # 3. è°ƒç”¨LLM
        llm_response = self._call_llm_for_analysis(prompt)
        if not llm_response:
            return self._get_minimal_defaults()
        
        # 4. è§£æç»“æœ
        return self._parse_analysis_result(llm_response)

